The values reported in Figure 1 and Tables S1-S18 in our paper are obtained by scoring the outputs of main.py in each of these subdirectories. Before running each main.py file, you must:

1. Populate the corresponding api.env file with the necessary information, including a valid API key, as described below.
2. Copy the 40 preprocessed text files that make up the test set into the corresponding /test-set/ directory. These files can be obtained by running the scripts and following the instructions included in the [/download_papers/test_set/](https://github.com/benjww/text_mining_prep/tree/main/download_papers/test_set/) and [/preprocessing/test_set/](https://github.com/benjww/text_mining_prep/tree/main/preprocessing/test_set/) directories in our sibling [text_mining_prep repository](https://github.com/benjww/text_mining_prep/tree/main/).

Models GPT-3.5-Turbo (version 0125), GPT-4-Turbo (version 2024-04-09), GPT-4o-mini (version 2024-07-18), and GPT-4o (version 2024-08-06) are called using the Microsoft Azure OpenAI API. Documentation for this cloud service can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference). In the provided api.env files you must enter your desired model (AKA deployment ID), API key, API version, Azure endpoint URL, and organization ID. We provide the deployment IDs that we used in the provided api.env files; however, these IDs are specific to your Azure endpoint URL, and will not necessarily call the desired model version in your case. 
- In this study, we accessed the Microsoft Azure OpenAI API through the University of Michigan Information and Technology Services (U-M ITS). We were internally provided with a confidential API key and endpoint URL, and used an internal shortcode for the organization ID. Deployment IDs were also communicated to us by U-M ITS. 

Models Llama 3.1 8B, 70B, 405B, and 3.3 70B are called using the Amazon Bedrock service provided by Amazon Web Services. Documentation for this cloud service can be found [here](https://docs.aws.amazon.com/bedrock/). In the provided api.env files you must enter your desired model ID, AWS Region, and two confidential API keys that can be requested upon creation of an AWS account. Model IDs and supported AWS Regions can be found [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html). 

Deepseek-V3 is called using Fireworks AI. Documentation for this service can be found [here](https://docs.fireworks.ai/getting-started/introduction). The associated api.env file only requires two fields, namely a model ID which we provide and a confidential API key that can be requested upon creation of a Fireworks AI account. 
 
